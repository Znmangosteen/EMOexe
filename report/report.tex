\documentclass[conference]{IEEEtran}

  \usepackage{booktabs}
  \usepackage{listing}
  \usepackage{amsmath}
  \usepackage{algorithm}
  \usepackage{array}
  \usepackage{url}
  \usepackage{cite}
  \usepackage{complexity}
  \usepackage{algpseudocode}
   \usepackage{graphicx}
% \usepackage{algorithm}
  \ifCLASSINFOpdf
  
  \else
  
  \fi
  
  \hyphenation{op-tical net-works semi-conduc-tor}
  
  
  \begin{document}
  
  \title{Parallel Distributed Multi-objective Fuzzy Genetics-based Machine Learning \\ Mid Term Report}
  
  \author{\IEEEauthorblockN{Bowen Zheng, Shijie Chen, Shuxin Wang}
  \IEEEauthorblockA{Department of Computer Science and Engineering\\
  Southern University of Science and Technology\\
  Shenzhen, Guangdong, China\\}
  }
  
  \maketitle
  
  \begin{abstract}
  In the second period of the project, we finished the design of fuzzy classifiers, GBML framework and the asynchronous parallel distributed system. We have implemented each part respectively and are working to integrate them together.
  \end{abstract}
  \IEEEpeerreviewmaketitle
  
  \section{Introduction}
  In this project, we aim to build a parallel distributed implementation of a multi-objective genetics based machine learning(GBML) algorithm. We choose a specific problem of a three-objective fuzzy rule-based classifier and fit it into a hybrid GBML framework. Then we develop a parallel mechanism to accelerate computation.

  Code of the three parts have been completed. We will integrate them together, fix bugs and run some test problems in the next stage.

  \section{Fuzzy Rule-based Classifiers}
  The design and implementation of fuzzy classifier is based on \cite{ishibuchi2007analysis}.
  \subsection{Fuzzy Rules}
  We use the following "if-then" rules:
  $$Rule\;R_q:\;if\;x_{pi}\;is\;A_{qi},\;i\in [1,n]\;then\;Class\;C_q\;with\;CF_q$$

  \begin{table}[H]
    \caption{Notation}
    \centering
      \begin{tabular}{cccc}
      \toprule
      Variable&Name\\
      \midrule
      $n$&dimension of patterns\\
      $M$&number of classes of patterns\\
      $S$&a fuzzy classifier\\
      $I$&number of membership functions\\
      $x_p$&a pattern vector\\
      $x_{pi}$&attribute of $x_p$ value on $i$-th dimension\\
      $A_{qi}$&antecedent fuzzy set\\
      $\mu_{A_{qi}}(x)$&membership function of $A_{qi}$\\
      $\mu_{A_{q}}(x_p)$&compatibility grade of $x_p$ with $A_q$\\
      $A_q$&antecedent part of $q$-th rule\\
      
      $R_q$&$q$-th rule\\
      $C_q$&consequent class for $q$-th rule\\
      $CF_q$&rule weight for $q$-th rule\\
  \bottomrule
  \end{tabular}
  \label{table:1}
  \end{table}
  Input vectors are normalized to a hypercube $[0,1]^n$ using the following equation:
  $$x_{pi}=\frac{x_{pi}-min(x_{i})}{max(x_{i})-min(x_{i})}$$

  The antecedent fuzzy set contains a membership function of the form:
  $$\mu_{A_{qi}}(x_{pi}) =max\{1-\frac{|a-x_{pi}|}
  {b},0\}$$
  $$a=\frac{k-1}{K-1}$$
  $$b=\frac{1}{K-1}$$
  Where K is the number of intervals of fuzzy set $A_{qi}$ and k is the order of the interval. As shown in fig.\ref{fig:mfunc}. The $don't\;care$ condition is a constant function with value 1. As we will discuss in classification process, the feature $x_{pi} $ with $A_{qi}$ being $don't\;care$ is ignored.

  \begin{figure}[H]
    \centering
    \includegraphics[width = 0.4\textwidth]{figures/mfunc.png}
    \caption{Membership functions with at most 3 intervals}
    \label{fig:mfunc}
  \end{figure}

  The antecedent part of $R_q$ is a set of antecedent fuzzy sets. $$A_q = \{A_{qi}|i\in [1,n]\}$$

  We define the compatibility grade of an pattern $x_p$ with rule $R_q$ as
  $$\mu_{A_q}(x_p) = \prod_{i = 1}^{n}\mu_{A_{qi}}(x_{pi})$$

  Then we determine the consequent class $C_q$ and rule weight $CF_q$ using training patterns as follows:

  First, we compute the confidence of fuzzy rule $R_q$ to each class $c(A_q \Rightarrow Class\;h), h \in[1,M]$
  $$c(A_q \Rightarrow Class\; h) = \frac{\sum\limits_{x_p \in Class\;h}\mu_{A_q}(x_p)}{\sum\limits_{p=1}^m\mu_{A_q}(x_p)}$$

  Then the consequent class $C_q$ as the class with which $R_q$ has the largest confidence.
  $$c(A_q \Rightarrow Class\;C_q)=max\{c(A_q \Rightarrow Class\;h)|h \in [1,M]\}$$
  
  At last, rule weight of $R_q$ is given by the difference between its consequent class and other classes.
  $$CF_q = c(A_q\Rightarrow Class\;C_q)-\sum\limits_{h=1, h\neq C_q}^{M}c(A_q\Rightarrow Class \; h)$$

  Rule weight shows the quality of a classification result given by a fuzzy rule. If a rule has negative rule weight, its abandoned.
  
  \subsection{Fuzzy Classifier}
  
  A fuzzy classifier $S$ is a set of fuzzy rules. Given an input pattern $x_p$, the classification result $C_w$ is produced by a winning rule $R_w$, chosen as follows:
  $$\mu_{A_w}(x_p) \cdot CF_w = max\{\mu_{A_q}(x_p)\cdot CF_q|R_q\in S\}$$
  \subsection{Generate Fuzzy Rule From Training Patterns}
  At the initiation stage of our GBML algorithm, the population, set of fuzzy classifiers, is created from training data. We can use certain training patterns to create a fuzzy classifier and repeat the process to create a set of classifiers.
  
  Each rule $R_t$ in each classifier is generated from a training pattern $x_t$ as follows:
% \begin{itemize}
    % \item

  We choose an antecedent fuzzy set according to each $x_{ti}$ and form the antecedent part $A_t$ of rule $R_t$ so that $x_{t}$ has the largest compatibility:
  $$ \A_{ti} = \underset{\mu_j}{\arg\max}\{\mu_{j}(x_{ti})\},j \in [1,I]$$
    % \item 
  
  Then, randomly change the antecedent fuzzy set to $dont't\;care$ according to a pre-specified probability $p_{dc}$. This step prevents overfitting and improves generalizing ability of fuzzy rules.
% \end{itemize}
  \section{Hybrid Genetics-based Machine Learning Framework}
	 \par After designing the fuzzy classifier, the next step is approaching our three objectives: correctly classified training patterns, number of fuzzy rules and total number of antecedent conditions in the fuzzy classifier S. Currently we only implement the two objects.
	 \par
	 We use a hybrid GBML algorithm to find the non-dominated rule sets of this problem. The genetic algorithm we used is basically following \cite{ISHIBUCHI20074}. This hybrid GBML is implemented in the framework of non-dominated sorting genetic algorithm II(NSGA-II) and it's a Pittsburgh-style algorithm. Besides, it use Michigan-style GBML to change a rule as the mutation operation. 
	 \par
	In our genetic algorithm, the population is a set of fuzzy classifier, and the individual is a single fuzzy classifier (i.e., a set of fuzzy rules). The basic steps of our algorithm is followed. First step, we'll initialize our population with size $N_pop$ using training data. Second step, we'll generate $N_pop$ offspring by implementing selecting and crossover operators to two parents, and do mutate on the offspring. Third step, the origin and offspring population will be combined together and we only keep the first $N_pop$ individuals after the population sorting by Pareto ranking and crowding measure, which will be introduced later. Finally, we'll check whether it have reached the stop condition,and if not we'll go back to the second step.
	 
	 \subsection{NSGA-II}
	 \par
	 NSGA-II is introduced detailedly in \cite{996017}, which is a common algorithm in multi-objective optimizing problem. I think the characters of NSGA-II algorithm comparing with a ordinary genetic algorithm is that it uses elite-preserving, Pareto ranking, and crowding measure. Elite-preserving is a strategy that can reserve outstanding gene by keeping the individual containing such a gene. Pareto ranking and crowding measure is used to compare individuals in the three-objective situation. We'll introduce the three futures respectively.
	 \subsubsection{Elite-preserving}
	 \par
	 Elite-preserving is a replacing strategy at the end of a generation. The basic idea is that we only replace individuals with bad performance in original population by the outstanding offspring, and we keep preeminent parents to reserve their high performance gene. And we measure individuals' performances using Pareto ranking and crowding measure.
	 
	 
	 \subsubsection{Pareto ranking}
	 \par
	 Pareto-optimal front is the solution set that we want to find in the multi-objective optimizing problem, and its definition can been seen at \cite{ISHIBUCHI20074}. In the multi-objective optimizing problem, individual in Pareto-optimal front can be seen as the "best" solution. We'll give these individuals the highest Pareto ranking. If we remove the individuals in Pareto-optimal front, we can find another Pareto-optimal front of the remaining individuals. Then we'll give the second front a lower Pareto ranking than previous one, and we continue do such iteration until every individual gets their Pareto ranking. And we can measure individuals' performance by their Pareto ranking. If individuals have same ranking number, we'll compare them by using crowding measure.
	 
	 \subsubsection{Crowding measure}
	 \par
	 Crowding measure can help us finding a more diverse Pareto set. As we wanting to find a more homogeneous solutions set, we'd like to choose the individual in a less crowded area. To estimate the density of a individual in a population, we calculate the sum of the distances to neighbor individuals on both side and for each objective. And shorter the distance is, more crowded area the individual is in.     
	 
	 \subsection{Michigan-style GBML}
	 \par
	 Different Pittsburgh-style algorithm, Michigan-style GBML see a single fuzzy rule as a population and see the membership function as an individual. So we use Michigan-style GBML as a mutation operator acting on a single rule to enrich the diversity of our population. 
	 \subsection{Validation}
	 After implementing our program, we validate its reliability using iris data set and it have successfully pass the feasibility test. The results are followed.
	 \\
	 \begin{tabular}{cccccc}
	 	\hline
	 	Number of rules& 1& 2&3&4&5\\
	 	\hline
	 	NO. of correctly classified pattern& 49& 99&132&134&137\\
	 	\hline
	 	error rate&0.67&0.34&0.12&0.11&0.09\\
	 	\hline
	 \end{tabular}

 \begin{figure}[H]
 	\centering
 	\includegraphics[width=0.5\textwidth]{iris.png}
 	\caption{Handwrite}\label{fig:digit}
 \end{figure}
  \section{Asynchronous Parallel Distributed System Design}


  \section{Contribution}
    \begin{itemize}
    \item Bowen Zheng - Design \& Implementation of parallel system
    \item Shijie Chen - Design \& Implementation of fuzzy classifier, Design of parallel system
    \item Shuxin Wang - Design \& Implementation of Hybrid GBML framework
    \end{itemize}
    

  \section*{Acknowledgment}


\bibliographystyle{ieeetr}
\bibliography{ref}
% that's all folks
\end{document}


  