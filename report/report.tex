\documentclass[conference]{IEEEtran}

  \usepackage{booktabs}
  \usepackage{listing}
  \usepackage{amsmath}
  \usepackage{algorithm}
  \usepackage{array}
  \usepackage{url}
  \usepackage{cite}
  \usepackage{complexity}
  \usepackage{algpseudocode}
   \usepackage{graphicx}
   \usepackage{enumerate}
   \usepackage{subcaption}
% \usepackage{algorithm}
  \ifCLASSINFOpdf
  
  \else
  
  \fi
  
  \hyphenation{op-tical net-works semi-conduc-tor}
  
  
  \begin{document}
  
  \title{Parallel Distributed Multi-objective Fuzzy Genetics-based Machine Learning \\ Mid Term Report}
  
  \author{\IEEEauthorblockN{Bowen Zheng, Shijie Chen, Shuxin Wang}
  \IEEEauthorblockA{Department of Computer Science and Engineering\\
  Southern University of Science and Technology\\
  Shenzhen, Guangdong, China\\}
  }
  
  \maketitle
  
  \begin{abstract}
  In the second period of the project, we finished the design of fuzzy classifiers, GBML framework and the asynchronous parallel distributed system. We have implemented each part respectively and are working to integrate them together.
  \end{abstract}
  \IEEEpeerreviewmaketitle
  
  \section{Introduction}
  In this project, we aim to build a parallel distributed implementation of a multi-objective genetics based machine learning(GBML) algorithm. We choose a specific problem of a three-objective fuzzy rule-based classifier and fit it into a hybrid GBML framework. Then we develop a parallel mechanism to accelerate computation.

  Code of the three parts have been completed. We will integrate them together, fix bugs and run some test problems in the next stage.

  \section{Fuzzy Rule-based Classifiers}
  The design and implementation of fuzzy classifier is based on \cite{ishibuchi2007analysis}.
  \subsection{Fuzzy Rules}
  We use the following "if-then" rules:
  $$Rule\;R_q:\;if\;x_{pi}\;is\;A_{qi},\;i\in [1,n]\;then\;Class\;C_q\;with\;CF_q$$

  \begin{table}[H]
    \caption{Notation}
    \centering
      \begin{tabular}{cccc}
      \toprule
      Variable&Name\\
      \midrule
      $n$&dimension of patterns\\
      $M$&number of classes of patterns\\
      $S$&a fuzzy classifier\\
      $I$&number of membership functions\\
      $x_p$&a pattern vector\\
      $x_{pi}$&attribute of $x_p$ value on $i$-th dimension\\
      $A_{qi}$&antecedent fuzzy set\\
      $\mu_{A_{qi}}(x)$&membership function of $A_{qi}$\\
      $\mu_{A_{q}}(x_p)$&compatibility grade of $x_p$ with $A_q$\\
      $A_q$&antecedent part of $q$-th rule\\
      
      $R_q$&$q$-th rule\\
      $C_q$&consequent class for $q$-th rule\\
      $CF_q$&rule weight for $q$-th rule\\
  \bottomrule
  \end{tabular}
  \label{table:1}
  \end{table}
  Input vectors are normalized to a hypercube $[0,1]^n$ using the following equation:
  $$x_{pi}=\frac{x_{pi}-min(x_{i})}{max(x_{i})-min(x_{i})}$$

  The antecedent fuzzy set contains a membership function of the form:
  $$\mu_{A_{qi}}(x_{pi}) =max\{1-\frac{|a-x_{pi}|}
  {b},0\}$$
  $$a=\frac{k-1}{K-1}$$
  $$b=\frac{1}{K-1}$$
  Where K is the number of intervals of fuzzy set $A_{qi}$ and k is the order of the interval. As shown in fig.\ref{fig:mfunc}. The $don't\;care$ condition is a constant function with value 1. As we will discuss in classification process, the feature $x_{pi} $ with $A_{qi}$ being $don't\;care$ is ignored.

  \begin{figure}[H]
    \centering
    \includegraphics[width = 0.4\textwidth]{figures/mfunc.png}
    \caption{Membership functions with at most 3 intervals}
    \label{fig:mfunc}
  \end{figure}

  The antecedent part of $R_q$ is a set of antecedent fuzzy sets. $$A_q = \{A_{qi}|i\in [1,n]\}$$

  We define the compatibility grade of an pattern $x_p$ with rule $R_q$ as
  $$\mu_{A_q}(x_p) = \prod_{i = 1}^{n}\mu_{A_{qi}}(x_{pi})$$

  Then we determine the consequent class $C_q$ and rule weight $CF_q$ using training patterns as follows:

  First, we compute the confidence of fuzzy rule $R_q$ to each class $c(A_q \Rightarrow Class\;h), h \in[1,M]$
  $$c(A_q \Rightarrow Class\; h) = \frac{\sum\limits_{x_p \in Class\;h}\mu_{A_q}(x_p)}{\sum\limits_{p=1}^m\mu_{A_q}(x_p)}$$

  Then the consequent class $C_q$ as the class with which $R_q$ has the largest confidence.
  $$c(A_q \Rightarrow Class\;C_q)=max\{c(A_q \Rightarrow Class\;h)|h \in [1,M]\}$$
  
  At last, rule weight of $R_q$ is given by the difference between its consequent class and other classes.
  $$CF_q = c(A_q\Rightarrow Class\;C_q)-\sum\limits_{h=1, h\neq C_q}^{M}c(A_q\Rightarrow Class \; h)$$

  Rule weight shows the quality of a classification result given by a fuzzy rule. If a rule has negative rule weight, its abandoned.
  
  \subsection{Fuzzy Classifier}
  
  A fuzzy classifier $S$ is a set of fuzzy rules. Given an input pattern $x_p$, the classification result $C_w$ is produced by a winning rule $R_w$, chosen as follows:
  $$\mu_{A_w}(x_p) \cdot CF_w = max\{\mu_{A_q}(x_p)\cdot CF_q|R_q\in S\}$$
  \subsection{Generate Fuzzy Rule From Training Patterns}
  At the initiation stage of our GBML algorithm, the population, set of fuzzy classifiers, is created from training data. We can use certain training patterns to create a fuzzy classifier and repeat the process to create a set of classifiers.
  
  Each rule $R_t$ in each classifier is generated from a training pattern $x_t$ as follows:
% \begin{itemize}
    % \item

  We choose an antecedent fuzzy set according to each $x_{ti}$ and form the antecedent part $A_t$ of rule $R_t$ so that $x_{t}$ has the largest compatibility:
  $$ \A_{ti} = \underset{\mu_j}{\arg\max}\{\mu_{j}(x_{ti})\},j \in [1,I]$$
    % \item 
  
  Then, randomly change the antecedent fuzzy set to $dont't\;care$ according to a pre-specified probability $p_{dc}$. This step prevents overfitting and improves generalizing ability of fuzzy rules.
% \end{itemize}
  \section{Hybrid Genetics-based Machine Learning Framework}
  After designing the fuzzy classifier, the next step is to approach our three objectives: maximizing number of correctly classified training patterns, minimizing number of fuzzy rules and minimizing total number of antecedent conditions in the fuzzy classifier S. Our current implementation only consider fist two objectives.
	 
	 We use a hybrid GBML algorithm to find the non-dominated rule sets. The genetic algorithm we used is based on \cite{ishibuchi2007analysis}. This hybrid GBML is implemented in the framework of non-dominated sorting genetic algorithm II(NSGA-II) and it's a Pittsburgh-style algorithm. Besides,it use Michigan-style GBML to change a rule as the mutation operation. 
	 
  In our genetic algorithm, the population is a set of fuzzy classifiers, and each individual is a single fuzzy classifier. The basic steps of our algorithm are:
  \begin{enumerate}
  \item Initialize the population with size $N_{pop}$ using training data.
  \item Generate $N_{pop}$ offsprings by crossover operation. Then apply mutation on the offsprings.
  \item Combine the original population and offsprings and keep the best $N_{pop}$ individuals according to Pareto ranking and crowding measure.
  \item Repeat 2) and 3) until the stopping criterion is met.
  \end{enumerate}
  
	 
	 \subsection{NSGA-II}
	 
   NSGA-II, which is a common algorithm for multi-objective optimizing problems, is introduced in detail in \cite{996017}. Compared to ordinary genetic algorithms, it uses elite-preserving, Pareto ranking, and crowding measure to control the population. Elite-preserving is a strategy that we keep the individuals with the best performance in each iteration. Pareto ranking and crowding measure are used for non-dominated sorting in the three-objective situation. Individuals are ranked by Pareto-ranking first. If two individuals have the same Pareto-ranking, on with lower crowding measure is preferred.
   
	 \subsubsection{Elite-preserving}
	 
	 Elite-preserving is a replacing strategy. The idea is that we replace individuals with bad performance in the original population by the outstanding offsprings, and keep preeminent parents to reserve their high performance gene. 
	 
	 \subsubsection{Pareto ranking}
	 
   Pareto-optimal front is the solution set that we want to find in the multi-objective optimizing problem, and its definition can been seen at \cite{ishibuchi2007analysis}. Solutions are divided into  non-dominated solution sets where there is no solution dominated by another solution in the set. Pareto ranking are assigned as the number of non-nominated solution sets that dominates it. In particular, the Pareto-optimal front is not dominated and is seen as the set of best solutions.
   
  %  We'll give these individuals the highest Pareto ranking. If we remove the individuals in Pareto-optimal front, we can find another Pareto-optimal front of the remaining individuals. Then we'll give the second front a lower Pareto ranking than previous one, and we continue do such iteration until every individual gets their Pareto ranking. And we can measure individuals' performance by their Pareto ranking. If individuals have same ranking number, we'll compare them by using crowding measure.
	 
	 \subsubsection{Crowding measure}
	 
	 Crowding measure is used to find a more evenly distributed Pareto set and maintain the diversity of the population. This will enhance the algorithm's ability to escape local optimal. An individual's crowding measure is computed as the sum of its distances to its neighboring vectors on both positive and negative directions for each objective. Priority is given to individuals with larger crowding measure.
	 
	 \subsection{Michigan-style GBML}
	 
   Different from the Pittsburgh-style algorithm, Michigan-style GBML sees a single fuzzy rule as the population and sees the membership function as an individual. We use Michigan-style GBML as a mutation operator acting on a single rule to add diversity to the population. 
   
  
   
  \section{Asynchronous Parallel Distributed System Design}
  We propose an asynchronous parallel distributed system as shown in fig.\ref{fig:fig} to accelerate our GBML algorithm. The model includes 3 parts: dataset distributor, local island worker and population migration control. (For convenience, slave process will be called as island)


  \begin{figure}[H]
    \centering
    \begin{subfigure}{0.45\linewidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/master.png}
      \caption{master process}
      \label{fig:sfig1}
    \end{subfigure}%
    \begin{subfigure}{0.45\linewidth}
      \centering
      \includegraphics[width=0.9\linewidth]{figures/slave.png}
      \caption{slave process}
      \label{fig:sfig2}
    \end{subfigure}
    
    \caption{Master-slave Design}
    \label{fig:fig}
    \end{figure}
  \subsection{Master Process}
  \begin{enumerate}
  \item read dataset and initialize the dataset pool
  \item distribute $N_land$ simples to each P islands through dataset distributor layer.
  \item start P slave processes, which each of them works on a cpu core.
  \item wait for all islands run its “init” part and return their population. Use these populations, build an initialized population pool.
  \item generate new population for each island and start its "main" part
  \item Listening each island, exchange(migrate) population when an island run a certain iterations $I_update$.
  \item terminate when all listener receive the stop message.
  \end{enumerate}
  \subsection{Slave Processes}
  Init:
  \begin{enumerate}
  \item Receive training dataset from the master process.
  \item Initialize population from the dataset.
  \item Run the GBML for $I_init$ iterations.
  \item upload population to the master process.
  \end{enumerate}

  Main:
  \begin{enumerate}
  \item Run the GBML for $I_update$ iterations.
  \item Upload the population to the master process and wait for the new population from migration control layer.
  \item Terminates if stopping criterion is met and sent a ending message to the master process.
  \end{enumerate}

	\subsection{Dataset Distributor:}
	The dataset distributor is responsible for the dataset distribution process, which delivers a portion of the training dataset to each island as local training data. During this process, the distributor randomly picks $N_{land}$ data from the dataset pool and send it to a slave process. Repeat this procedure for each island.
  \subsection{Population Migration Control:}
  In the initialization stage, the population migration control layer send training data to each island and awaits returned population. When it receives returned population from all islands, it put together these populations and form a large population pool. 
	
	After the initialization, this layer delivers new population to each island on request and activates their “main” part. The new population is generated by randomly selecting $ N_{pop_i}$ individuals from the current population pool. Selected data will be deleted from the pool so that the size of the pool remains unchanged.
	
  \subsection{Asynchronous Population Migration}
  After initialization, each island will run the GBML algorithm independently for $I_{update}$ iterations and then updates its population through population migration control layer. In this way, population migration of each island is independent. We expect that our model could improve parallel efficiency especially when the update interval of the islands are different. This will also allow us to run the GBML algorithm on different CPUs with different configurations to further enhance search ability.
  
  
  \section{Computational Experiments}

	 Our current implementation has integrated the hybrid GBML framework and the fuzzy rule-based classifier. Feasibility test is done on the Iris Flower dataset. The result is as follows:

 \begin{figure}[H]
 	\centering
 	\includegraphics[width=0.4\textwidth]{figures/iris.png}
 	\caption{Pareto front obtained on Iris Flowers}\label{fig:digit}
 \end{figure}

 We will fix bugs (if any) of the fuzzy GBML algorithm and then integrate the algorithm into our proposed asynchronous parallel framework. 

  \section{Contribution}
  
    \begin{itemize}
    \item Bowen Zheng - Design \& Implementation of parallel system
    \item Shijie Chen - Design \& Implementation of fuzzy classifier, Design of parallel system
    \item Shuxin Wang - Design \& Implementation of Hybrid GBML framework
    \end{itemize}
    



\bibliographystyle{ieeetr}
\bibliography{ref}
% that's all folks
\end{document}


  